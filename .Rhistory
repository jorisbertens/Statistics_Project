fviz_screeplot(res.pca, addlabels = TRUE, ylim = c(0, 50))
res.pca = PCA(df_features, graph = TRUE)
res.pca = PCA(df_features, graph = FALSE)
fviz_screeplot(res.pca, addlabels = TRUE, ylim = c(0, 50))
var = get_pca_var(res.pca)
View(var)
head(var$cos2)
head(var$cos2)
head(var$coord)
head(var$contrib)
head(var$coord, 4)
fviz_pca_var(res.pca, col.var = "black")
library("corrplot")
corrplot(var$cos2, is.corr=FALSE)
fviz_cos2(res.pca, choice = "var", axes = 1:2)
# Color by cos2 values: quality on the factor map
fviz_pca_var(res.pca, col.var = "cos2",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE # Avoid text overlapping
)
# Change the transparency by cos2 values
fviz_pca_var(res.pca, alpha.var = "cos2")
# Color by cos2 values: quality on the factor map
fviz_pca_var(res.pca, col.var = "cos2",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE # Avoid text overlapping
)
head(var$contrib, 4)
corrplot(var$contrib, is.corr=FALSE)
# Contributions of variables to PC2
fviz_contrib(res.pca, choice = "var", axes = 2, top = 10)
# Contributions of variables to PC2
fviz_contrib(res.pca, choice = "var", axes = 2, top = 10)
# Contributions of variables to PC1
fviz_contrib(res.pca, choice = "var", axes = 1, top = 10)
# Contributions of variables to PC2
fviz_contrib(res.pca, choice = "var", axes = 2, top = 10)
# Contributions of variables to PC3
fviz_contrib(res.pca, choice = "var", axes = 3, top = 10)
# Contributions of variables to PC4
fviz_contrib(res.pca, choice = "var", axes = 4, top = 10)
# Contributions of variables to PC5
fviz_contrib(res.pca, choice = "var", axes = 5, top = 10)
fviz_contrib(res.pca, choice = "var", axes = 1:5, top = 10)
# Contributions of variables to PC1
fviz_contrib(res.pca, choice = "var", axes = 1, top = 10)
fviz_pca_ind(res.pca)
fviz_pca_ind(res.pca, col.ind = "cos2",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE # Avoid text overlapping (slow if many points)
)
fviz_pca_ind(res.pca, pointsize = "cos2",
pointshape = 21, fill = "#E7B800",
repel = TRUE # Avoid text overlapping (slow if many points)
)
View(df)
plot(fit_ward, labels=df$geo.time)
rect.hclust(fit_ward, k=3, border="red")
get_full_data<-function(data, sep=","){
df <- read.csv(data, header = TRUE, sep)
}
rm(list=ls())
library(dendextend)
library(FactoMineR)
library(factoextra)
library(corrplot)
library(cluster)
library(gdata)
library(dplyr)
library(reshape2)
get_full_data<-function(data, sep=","){
df <- read.csv(data, header = TRUE, sep)
}
df = get_full_data("Data/full_dataset.csv")
df = df[(df$variable=="X2017"),]
df2 = df %>% select(4:15)
d<-dist(df2)
# Elbow method
fviz_nbclust(df2, hcut, method = "wss") +
geom_vline(xintercept = 4, linetype = 2)+
labs(subtitle = "Elbow method")
fviz_nbclust(df2, hcut, method = "silhouette")+
labs(subtitle = "Silhouette method")
## Impute by mean
for(i in 1:ncol(df2)){
df2[is.na(df2[,i]), i] <- mean(df2[,i], na.rm = TRUE)
}
# Elbow method
fviz_nbclust(df2, hcut, method = "wss") +
geom_vline(xintercept = 4, linetype = 2)+
labs(subtitle = "Elbow method")
fviz_nbclust(df2, hcut, method = "silhouette")+
labs(subtitle = "Silhouette method")
fviz_nbclust(df2, hcut, nstart = 25,  method = "gap_stat", nboot = 50)+
labs(subtitle = "Gap statistic method")
library("NbClust")
nb <- NbClust(df2, distance = "euclidean", min.nc = 2,
max.nc = 10, method = "kmeans")
fviz_nbclust(nb)
fit_ward<-hclust(d,method="ward.D")
plot(fit_ward)
rect.hclust(fit_ward, k=3, border="red")
groups_ward <- cutree(fit_ward, k=3)
df$groups_ward<-groups_ward
table(df$geo.time,df$groups_ward)
plot(fit_ward, labels=df$geo.time)
rect.hclust(fit_ward, k=3, border="red")
rm(list=ls())
library(FactoMineR)
library(factoextra)
http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/
get_full_data<-function(data, sep=","){
df <- read.csv(data, header = TRUE, sep)
}
df = get_full_data("Data/full_dataset.csv")
df = df[(df$variable=="X2017"),]
# get rid of unnecessary stuff
df2 = df %>% select(4:15)
# get rid of the target variable
df_features = df2[-8]
PCA(df_features, scale.unit = TRUE, ncp = 5, graph = TRUE)
res.pca = PCA(df_features, graph = FALSE)
print(res.pca)
eig.val = get_eigenvalue(res.pca)
eig.val
fviz_screeplot(res.pca, addlabels = TRUE, ylim = c(0, 50))
var = get_pca_var(res.pca)
head(var$coord)
head(var$cos2)
head(var$contrib)
head(var$coord, 4)
fviz_pca_var(res.pca, col.var = "black")
library("corrplot")
corrplot(var$cos2, is.corr=FALSE)
fviz_cos2(res.pca, choice = "var", axes = 1:2)
fviz_cos2(res.pca, choice = "var", axes = 1:4)
# Color by cos2 values: quality on the factor map
fviz_pca_var(res.pca, col.var = "cos2",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE # Avoid text overlapping
)
# Change the transparency by cos2 values
fviz_pca_var(res.pca, alpha.var = "cos2")
head(var$contrib, 4)
library("corrplot")
corrplot(var$contrib, is.corr=FALSE)
# Contributions of variables to PC1
fviz_contrib(res.pca, choice = "var", axes = 1, top = 10)
# Contributions of variables to PC2
fviz_contrib(res.pca, choice = "var", axes = 2, top = 10)
# Contributions of variables to PC3
fviz_contrib(res.pca, choice = "var", axes = 3, top = 10)
# Contributions of variables to PC4
fviz_contrib(res.pca, choice = "var", axes = 4, top = 10)
# Contributions of variables to PC5
fviz_contrib(res.pca, choice = "var", axes = 5, top = 10)
# Contributions of variables to PC4
fviz_contrib(res.pca, choice = "var", axes = 4, top = 10)
# Contributions of variables to PC5
fviz_contrib(res.pca, choice = "var", axes = 5, top = 10)
# Contributions of variables to PC3
fviz_contrib(res.pca, choice = "var", axes = 3, top = 10)
# Contributions of variables to PC4
fviz_contrib(res.pca, choice = "var", axes = 4, top = 10)
# Contributions of variables to PC3
fviz_contrib(res.pca, choice = "var", axes = 3, top = 10)
# Contributions of variables to PC2
fviz_contrib(res.pca, choice = "var", axes = 2, top = 10)
# Contributions of variables to PC1
fviz_contrib(res.pca, choice = "var", axes = 1, top = 10)
fviz_pca_ind(res.pca)
fviz_pca_ind(res.pca, col.ind = "cos2",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE # Avoid text overlapping (slow if many points)
)
fviz_pca_ind(res.pca, pointsize = "cos2",
pointshape = 21, fill = "#E7B800",
repel = TRUE # Avoid text overlapping (slow if many points)
)
rm(list=ls())
library(dplyr)
library(plyr)
# Import the datasets
df1 = get_data("Data/beds/beds_1000.csv")
df2 = get_data("Data/beds/beds_tourist.csv")
df3 = get_data("Data/companies/tot_companies.csv")
df4 = get_data("Data/culture/museum_vis.csv")
df5 = get_data("Data/employment/eco_active.csv")
df6 = get_data("Data/other_nationals/foreigners.csv")
df7 = get_data("Data/other_nationals/foreign_eu.csv")
get_data<-function(data, sep=","){
df <- read.csv(data, header = TRUE, sep)
}
# Import the datasets
df1 = get_data("Data/beds/beds_1000.csv")
setwd("~/notebooks/2_Semester/Stastistics/Statistics_Project/old")
# Import the datasets
df1 = get_data("Data/beds/beds_1000.csv")
df2 = get_data("Data/beds/beds_tourist.csv")
df3 = get_data("Data/companies/tot_companies.csv")
df4 = get_data("Data/culture/museum_vis.csv")
df5 = get_data("Data/employment/eco_active.csv")
df6 = get_data("Data/other_nationals/foreigners.csv")
df7 = get_data("Data/other_nationals/foreign_eu.csv")
df8 = get_data("Data/other_nationals/foreign_non_eu.csv")
df9 = get_data("Data/population/population.csv")
df10 = get_data("Data/private_households/households.csv")
df11 = get_data("Data/transport/cost_pt.csv")
# Change common column name value to something else
df1 = change_col_name(df1,"Value","beds_1000")
# change columns names before the merge to stop any conflict from happening
change_col_name<-function(data,old_col,new_col){
names(data)[names(data) == old_col] <- new_col
return(data)
}
# function to join the dataset by two keys (TIME and CITIES)
join_data<-function(df1,df2){
df_new <- merge(df1,df2,by.x = c("geo.time","variable"),by.y = c("geo.time","variable"))
return(df_new)
}
# Change common column name value to something else
df1 = change_col_name(df1,"Value","beds_1000")
df2 = change_col_name(df2,"Value","beds_tourist")
df3 = change_col_name(df3,"Value","tot_companies")
df4 = change_col_name(df4,"Value","museum_vis")
df5 = change_col_name(df5,"Value","employed_tot")
df6 = change_col_name(df6,"Value","foreign_born")
df7 = change_col_name(df7,"Value","foreign_eu")
df8 = change_col_name(df8,"Value","foreign_non_eu")
df9 = change_col_name(df9,"Value","population")
df10 = change_col_name(df10,"Value","household")
df11 = change_col_name(df11,"Value","cost_pt")
df1 = df1[,-c(3)]
df2 = df2[,-c(3)]
df3 = df3[,-c(3)]
df4 = df4[,-c(3)]
df5 = df5[,-c(3)]
df6 = df6[,-c(3)]
df7 = df7[,-c(3)]
df8 = df8[,-c(3)]
df9 = df9[,-c(3)]
df10 = df10[,-c(3)]
df11 = df11[,-c(3)]
df9 =df9[,-c(4)]
# delete duplicate columns for the columns TIME and CITIES
df1 = df1 %>% distinct(TIME, CITIES, .keep_all = TRUE)
df2 = df2 %>% distinct(TIME, CITIES, .keep_all = TRUE)
df3 = df3 %>% distinct(TIME, CITIES, .keep_all = TRUE)
df4 = df4 %>% distinct(TIME, CITIES, .keep_all = TRUE)
df5 = df5 %>% distinct(TIME, CITIES, .keep_all = TRUE)
df6 = df6 %>% distinct(TIME, CITIES, .keep_all = TRUE)
df7 = df7 %>% distinct(TIME, CITIES, .keep_all = TRUE)
df8 = df8 %>% distinct(TIME, CITIES, .keep_all = TRUE)
df9 = df9 %>% distinct(TIME, CITIES, .keep_all = TRUE)
df10 = df10 %>% distinct(TIME, CITIES, .keep_all = TRUE)
df11 = df11 %>% distinct(TIME, CITIES, .keep_all = TRUE)
# Join the datasets
df_joined = join_data(df1,df2)
View(df1)
df_new <- merge(df1,df2,by.x = c("TIME","GEO"),by.y = c("geo.time","variable"))
join_data<-function(df1,df2){
df_new <- merge(df1,df2,by.x = c("TIME","GEO"),by.y = c("geo.time","variable"))
return(df_new)
}
# Join the datasets
df_joined = join_data(df1,df2)
View(df5)
df_new <- merge(df1,df2,by.x = c("TIME","CITIES"),by.y = c("geo.time","variable"))
join_data<-function(df1,df2){
df_new <- merge(df1,df2,by.x = c("TIME","CITIES"),by.y = c("geo.time","variable"))
return(df_new)
}
# Join the datasets
df_joined = join_data(df1,df2)
View(df3)
join_data<-function(df1,df2){
df_new <- merge(df1,df2,by.x = c("TIME","CITIES"),by.y = c("TIME","CITIES"))
return(df_new)
}
# Join the datasets
df_joined = join_data(df1,df2)
df_joined = join_data(df_joined, df3)
df_joined = join_data(df_joined, df4)
df_joined = join_data(df_joined, df5)
df_joined = join_data(df_joined, df6)
df_joined = join_data(df_joined, df7)
df_joined = join_data(df_joined, df8)
df_joined = join_data(df_joined, df9)
df_joined = join_data(df_joined, df10)
df_joined = join_data(df_joined, df11)
View(df_joined)
df = df[(df$TIME=="2017"),]
df_joined = df_joined[(df_joined$TIME=="2017"),]
View(df_joined)
sum(is.na(df_joined$TIME))
sum(is.na(df_joined$CITIES))
sum(is.na(df_joined$beds_1000))
sum(is.na(df_joined$beds_tourist))
sum(is.na(df_joined$tot_companies))
sum(is.na(df_joined$museum_vis))
sum(is.na(df_joined$employed_tot))
sum(is.na(df_joined$foreign_born))
sum(is.na(df_joined$foreign_eu))
sum(is.na(df_joined$foreign_non_eu))
sum(is.na(df_joined$population))
sum(is.na(df_joined$cost_pt))
# Replace ":" with NA
df_joined[df_joined==":"]<-NA
sum(is.na(df_joined$TIME))
sum(is.na(df_joined$CITIES))
sum(is.na(df_joined$beds_1000))
sum(is.na(df_joined$beds_tourist))
sum(is.na(df_joined$tot_companies))
sum(is.na(df_joined$museum_vis))
sum(is.na(df_joined$employed_tot))
sum(is.na(df_joined$foreign_born))
sum(is.na(df_joined$foreign_eu))
sum(is.na(df_joined$foreign_non_eu))
sum(is.na(df_joined$population))
sum(is.na(df_joined$cost_pt))
# Join the datasets
df_joined = join_data(df1,df2)
df_joined = join_data(df_joined, df3)
df_joined = join_data(df_joined, df4)
df_joined = join_data(df_joined, df5)
df_joined = join_data(df_joined, df6)
df_joined = join_data(df_joined, df7)
df_joined = join_data(df_joined, df8)
df_joined = join_data(df_joined, df9)
df_joined = join_data(df_joined, df10)
df_joined = join_data(df_joined, df11)
df_joined = df_joined[(df_joined$TIME=="2015"),]
# Replace ":" with NA
df_joined[df_joined==":"]<-NA
"""""
cols = colnames(df_joined)
for (i in cols){
i1 = as.name(i)
#print(i1)
print(sum(is.na(df_joined$i1)))
#is.na(df_joined$TIME)
}
"""""
sum(is.na(df_joined$TIME))
sum(is.na(df_joined$CITIES))
sum(is.na(df_joined$beds_1000))
sum(is.na(df_joined$beds_tourist))
sum(is.na(df_joined$tot_companies))
sum(is.na(df_joined$museum_vis))
sum(is.na(df_joined$employed_tot))
sum(is.na(df_joined$foreign_born))
sum(is.na(df_joined$foreign_eu))
sum(is.na(df_joined$foreign_non_eu))
sum(is.na(df_joined$population))
sum(is.na(df_joined$cost_pt))
## Get house price data
df2007 = get_data("Data/house_pricing/Rent2007clean.csv", sep = ";")
df_joined[complete.cases(df_joined), ]
df = df_joined[complete.cases(df_joined), ]
View(df)
setwd("~/notebooks/2_Semester/Stastistics/Statistics_Project")
get_data<-function(data, sep=","){
df <- read.csv(data, header = TRUE, sep)
}
rm(list=ls())
get_data<-function(data, sep=","){
df <- read.csv(data, header = TRUE, sep)
}
df1 = get_data("Data/EconomicFreedomIndex.csv")
df1 = get_data("Data/EconomicFreedomIndex.csv", sep=";")
View(df1)
df2 = get_data("HumanFreedomIndex.csv", sep=";")
df2 = get_data("HumanFreedomIndex.csv")
df2 = get_data("Data/HumanFreedomIndex.csv")
df2 = get_data("Data/HumanFreedomIndex.csv" sep";")
df2 = get_data("Data/HumanFreedomIndex.csv" sep=";")
df2 = get_data("Data/HumanFreedomIndex.csv", sep=";")
View(df2)
df2 = df2[(df2$X=="2016"),]
View(df2)
View(df1)
df2 = get_data("Data/HumanFreedomIndex.csv", sep=";")
df2 = df2[(df2$X=="2016"),]
df2 = get_data("Data/HumanFreedomIndex.csv", sep=";")
View(df2)
df2 = get_data("Data/HumanFreedomIndex.csv")
View(df2)
df2 = df2[(df2$X=="2016"),]
df2 = get_data("Data/HumanFreedomIndex.csv")
View(df2)
df2 = df2[(df2$Year=="2016"),]
View(df2)
df2 = get_data("Data/HumanFreedomIndex.csv")
df2 = df2[(df2$Year=="2016"),]
View(df2)
View(df2)
View(df1)
View(df1)
View(df2)
View(df2)
}
join_data<-function(df1,df2){
df_new <- merge(df1,df2,by.x = c("Country.Name"),by.y = c("Countries"))
return(df_new)
}
df_join = join_data(df1,df2)
View(df_join)
View(df_join)
df_join= df2[-2,-3,-4,-5]
df_join= df2[-2,-3,-4]
View(df_join)
df_join = join_data(df1,df2)
df_join= df_join[-2,-3,-4,-5]
df_join= df_join[-2,-3,-4]
View(df_join)
df_join = join_data(df1,df2)
df_join= df_join[-2]
View(df_join)
df_join= df_join[-2]
df_join= df_join[-2]
View(df_join)
df_join= df_join[-2]
df_join= df_join[-2]
df_join= df_join[-2]
df_join= df_join[-2]
View(df_join)
df_join[, -grep("Change$", colnames(df_join))]
df_join[, -grep("$Change", colnames(df_join))]
df_join[, -grep("Change//$", colnames(df_join))]
df_join[, -grep("Chang", colnames(df_join))]
df_try = df_join[, -grep("Chang", colnames(df_join))]
View(df_try)
df_join.columns
df_join.col
df_join.info()
df_try = df_join[, -grep("Chang", colnames(df_join))]
colnames(df_try)
View(df_try)
cols = c(1, 4 ,6:11, 18, 19, 23, 25, 48, 56, 57, 58, 66, 71, 77, 89, 95, 107, 109, 119, 128, 162, 157, 165, 169, 88, 55, 35)
df_try[,cols]
df_try = df_try[,cols]
View(df_try)
df_try = df_join[, -grep("Chang", colnames(df_join))]
cols = c(1, 4 ,6:11, 18, 19, 23, 25, 48, 56, 57, 58, 66, 71, 77, 89, 95, 107, 109, 119, 128, 162, 157, 165, 169, 88, 55, 35)
colnames(df_try)
cols = c(1, 4 ,6:11, 18, 19, 25, 48, 56, 57, 58, 66, 71, 77, 89, 95, 107, 109, 119, 128, 162, 157, 165, 169, 88, 55, 35, 23)
df_try = df_try[,cols]
View(df_try)
View(df_try)
# change - to NA
df_try[df_try=="-"]<-NA
View(df_try)
df_try[df_try=="N/A"]<-NA
# download df
write.csv(df_try, file = "Data/unemployment_analysis.csv")
df_join = join_data(df1,df2)
df_join = join_data(df1,df2)
df_join = join_data(df1,df2)
rm(list=ls())
get_data<-function(data, sep=","){
df <- read.csv(data, header = TRUE, sep)
}
df1 = get_data("Data/EconomicFreedomIndex.csv", sep=";")
df2 = get_data("Data/HumanFreedomIndex.csv")
df2 = df2[(df2$Year=="2016"),]
join_data<-function(df1,df2){
df_new <- merge(df1,df2,by.x = c("Country.Name"),by.y = c("Countries"))
return(df_new)
}
df_join = join_data(df1,df2)
# delete first columns
df_join= df_join[-2]
df_join= df_join[-2]
df_join= df_join[-2]
df_join= df_join[-2]
df_join= df_join[-2]
df_join= df_join[-2]
df_join= df_join[-2]
# delete columns that include "Chang"
df_try = df_join[, -grep("Chang", colnames(df_join))]
# manually selected columns
cols = c(1, 4 ,6:11, 18, 19, 25, 48, 56, 57, 58, 66, 71, 77, 89, 95, 107, 109, 119, 128, 162, 157, 165, 169, 88, 55, 35, 23)
df_try = df_try[,cols]
# change - to NA
df_try[df_try=="-"]<-NA
df_try[df_try=="N/A"]<-NA
# download df
write.csv(df_try, file = "Data/unemployment_analysis.csv")
rm(list=ls())
# use these two packages
library(FactoMineR)
library(factoextra)
# I basically followed this guide to do everything for the PCA
http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/
get_full_data<-function(data, sep=","){
df <- read.csv(data, header = TRUE, sep)
}
df = get_full_data("Data/unemployment_analysis.csv")
View(df)
View(df)
# get rid of unnecessary stuff
df2 = df %>% select(3:33)
View(df2)
# get rid of the target variable for to prepare the PCA of the feature variables
df_features = df2[-31]
PCA(df_features, scale.unit = TRUE, ncp = 5, graph = TRUE)
res.pca = PCA(df_features, graph = FALSE)
print(res.pca)
View(df_features)
sapply(df, class)
View(df_features)
df2 =  as.numeric(df2)
